{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqLXMpiz1CFTEo6dNl00+N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nobrainghost/SymptoChat/blob/main/transforming%26vectorgeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install together pinecone\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "#Download and import the disease_symptom_list.csv"
      ],
      "metadata": {
        "id": "RYxyrwyRiHpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The original dataset was transformed to create the new csv disease_symptom_list.csv which is a simple disease-symptoms mapping\n",
        "\n",
        "import pandas as pd\n",
        "df2=pd.read_csv('disease_symptom_list.csv')\n",
        "disease_symptom_pairs=[]\n",
        "\n",
        "for idx,row in df2.iterrows():\n",
        "    disease=row['disease']\n",
        "    symptoms=row['symptoms'].split(',')\n",
        "    disease_symptom_pairs.append((disease,symptoms))\n",
        "\n",
        "for disease,symptoms in disease_symptom_pairs[:5]:\n",
        "       print(f\"Disease: {disease}\\nSymptoms: {', '.join(symptoms)}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nKMN9WJi2Cjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df2=pd.read_csv('disease_symptom_list.csv')\n",
        "disease_symptom_pairs=[]\n",
        "\n",
        "for idx,row in df2.iterrows():\n",
        "    disease=row['disease']\n",
        "    symptoms=row['symptoms'].split(',')\n",
        "    disease_symptom_pairs.append((disease,symptoms))\n",
        "\n",
        "for disease,symptoms in disease_symptom_pairs[:5]:\n",
        "       print(f\"Disease: {disease}\\nSymptoms: {', '.join(symptoms)}\\n\")\n",
        "\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import time\n",
        "import together\n",
        "import pinecone\n",
        "from pinecone import Pinecone\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize API clients with\n",
        "#If executing you'll need to set these keys in your secrets\n",
        "#pinecone_key-->Key to your pinecone db (For Storing of the vectors)\n",
        "#TOGETHER_API_KEY-->Token to your TogetherAI account\n",
        "\n",
        "try:\n",
        "    client = together.Client()\n",
        "    pinecone_key = userdata.get('pinecone_key')\n",
        "    pc = Pinecone(api_key=pinecone_key)\n",
        "    index_name = \"llama-text-embed-v2-index\"\n",
        "    index = pc.Index(index_name)\n",
        "except Exception as e:\n",
        "    print(f\"Error during initialization: {e}\")\n",
        "\n",
        "#Current Version only uses the regular llms as I find a suitable domain specific model. Exact model deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\n",
        "def generate_description(disease, symptoms):\n",
        "    try:\n",
        "        prompt = f\"\"\"Give an in depth description of what is {disease} with its symptoms and any other necessary information for its diagnosis. Explain common indicators/signs and also discuss its symptoms. Some of its symptoms are {', '.join(symptoms)}.\n",
        "\n",
        "        Explain how it's diagnosed and any other relevant clinical information.\"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\",\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }],\n",
        "            max_tokens=5400,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating description for {disease}: {e}\")\n",
        "        retry_count = 0\n",
        "        max_retries = 3\n",
        "        while retry_count < max_retries:\n",
        "            retry_count += 1\n",
        "            print(f\"Retrying ({retry_count}/{max_retries}) after waiting {2**retry_count} seconds...\")\n",
        "            time.sleep(2**retry_count)  # Exponential backoff\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\",\n",
        "                    messages=[{\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt,\n",
        "                    }],\n",
        "                    max_tokens=5400,\n",
        "                    temperature=0.7\n",
        "                )\n",
        "                return response.choices[0].message.content\n",
        "            except Exception as retry_e:\n",
        "                print(f\"Retry {retry_count} failed: {retry_e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "#Used Pinecone to generate and store the embeddings using their provided \"llama-text-embed-v2\"\n",
        "def generate_embeddings(description):\n",
        "    try:\n",
        "        embeddings = pc.inference.embed(\n",
        "            model=\"llama-text-embed-v2\",\n",
        "            inputs=description,\n",
        "            parameters={\n",
        "                \"input_type\": \"passage\"\n",
        "            }\n",
        "        )\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating embeddings: {e}\")\n",
        "        retry_count = 0\n",
        "        max_retries = 3\n",
        "        while retry_count < max_retries:\n",
        "            retry_count += 1\n",
        "            print(f\"Retrying embeddings ({retry_count}/{max_retries}) after waiting {2**retry_count} seconds...\")\n",
        "            time.sleep(2**retry_count)  # Exponential backoff\n",
        "            try:\n",
        "                embeddings = pc.inference.embed(\n",
        "                    model=\"llama-text-embed-v2\",\n",
        "                    inputs=description,\n",
        "                    parameters={\n",
        "                        \"input_type\": \"passage\"\n",
        "                    }\n",
        "                )\n",
        "                return embeddings\n",
        "            except Exception as retry_e:\n",
        "                print(f\"Retry {retry_count} failed: {retry_e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "# Main processing loop\n",
        "for disease, symptoms in disease_symptom_pairs:\n",
        "    try:\n",
        "        print(f\"Processing disease: {disease}\")\n",
        "        description = generate_description(disease, symptoms)\n",
        "\n",
        "        if description is None:\n",
        "            print(f\"Skipping {disease} due to description generation failure\")\n",
        "            continue\n",
        "        #Remove the Thinking part from the response to reduce chances of hallucinations\n",
        "        if \"<think>\" in description and \"</think>\" in description:\n",
        "            description = re.sub(r\"<think>.*?</think>\", \"\", description, flags=re.DOTALL)\n",
        "            description = description.strip()\n",
        "\n",
        "        if description:\n",
        "            embeddings = generate_embeddings(description)\n",
        "            if embeddings is None:\n",
        "                print(f\"Skipping embeddings for {disease} due to embedding generation failure\")\n",
        "                continue\n",
        "\n",
        "            for e in embeddings:\n",
        "                try:\n",
        "                    vectors = [{\"id\": disease, \"values\": e['values'], \"metadata\": {\"symptoms\": ', '.join(symptoms)}}]\n",
        "                    print(vectors)\n",
        "                    index.upsert(vectors=vectors)\n",
        "                    print(f\"Successfully uploaded vector for: {disease}\")\n",
        "                except Exception as upsert_e:\n",
        "                    print(f\"Error upserting vector for {disease}: {upsert_e}\")\n",
        "                    retry_count = 0\n",
        "                    max_retries = 3\n",
        "                    while retry_count < max_retries:\n",
        "                        retry_count += 1\n",
        "                        print(f\"Retrying upsert ({retry_count}/{max_retries}) after waiting {2**retry_count} seconds...\")\n",
        "                        time.sleep(2**retry_count)\n",
        "                        try:\n",
        "                            index.upsert(vectors=vectors)\n",
        "                            print(f\"Successfully uploaded vector for: {disease} on retry {retry_count}\")\n",
        "                            break\n",
        "                        except Exception as retry_e:\n",
        "                            print(f\"Upsert retry {retry_count} failed: {retry_e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {disease}: {e}\")"
      ],
      "metadata": {
        "id": "dxD7Jvd1RKZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial attempt at using HuggingFace inference to generate the desriptions with microsoft/biogpt\n",
        "#Does not work\n",
        "\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "model_name=\"microsoft/biogpt\"\n",
        "hf_token=userdata.get('hf_token')\n",
        "api_url=f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {hf_token}\"\n",
        "}\n",
        "def generate_description_via_api(disease, symptoms):\n",
        "    input_text = f\"Give an in depth description of {disease} with its symptoms and any other necessary information for its diagnosis {', '.join(symptoms)}:\"\n",
        "\n",
        "    payload = {\n",
        "        \"inputs\": input_text,\n",
        "        \"parameters\": {\n",
        "            \"max_length\": 500,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_k\": 50\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        generated_description = result[0]['generated_text']\n",
        "        return generated_description\n",
        "    else:\n",
        "        print(\"Error:\", response.status_code, response.text)\n",
        "        return None\n",
        "\n",
        "# disease_description_pairs = []\n",
        "# for disease, symptoms in disease_symptom_pairs:\n",
        "#     description = generate_description_via_api(disease, symptoms)\n",
        "#     if description:\n",
        "#         disease_description_pairs.append((disease, description))\n",
        "#         print(f\"Disease: {disease}\\nGenerated Description: {description}\\n\")\n",
        "#     else:\n",
        "#         print(f\"Failed to generate description for {disease}\")\n",
        "\n",
        "disease_description_pairs = []\n",
        "for disease, symptoms in disease_symptom_pairs[:1]:  # Example with the first pair\n",
        "    description = generate_description_via_api(disease, symptoms)\n",
        "    if description:\n",
        "        disease_description_pairs.append((disease, description))\n",
        "        print(f\"Disease: {disease}\\nGenerated Description: {description}\\n\")\n"
      ],
      "metadata": {
        "id": "AOkXlE8SDjOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1tewcrB8Urb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}